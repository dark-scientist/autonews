# ğŸš— Auto News Intelligence

Automated pipeline for processing, classifying, and analyzing automobile news articles from HTML files.

## Quick Publish Flow

From repository root, run:

```bash
./scripts/update_dashboard.sh /path/to/new_urls.txt
```

This ingests URLs, runs the full backend pipeline, and publishes `results.json` to `streamlit-app/`.

## Features

- **URL Management**: Centralized master URL list with deduplication
- **Admin UI**: Streamlit interface for URL upload and pipeline execution
- **Auto-filtering**: Keyword-based filtering to identify automobile-related articles
- **SBERT Embeddings**: Semantic embeddings using sentence-transformers
- **8-Category Classification**: Zero-LLM classification with cluster reasoning
- **Smart Deduplication**: Find duplicate stories with coherence scoring and stray detection
- **GPT-4o-mini Summarization**: One summary per unique story (optional)
- **Interactive Dashboard**: Streamlit UI with visualizations and filters
- **Enhanced Metadata**: Cluster coherence scores, duplicate reasons, classification explanations

---

## ğŸ“ Project Structure

```
project-root/
â”‚
â”œâ”€â”€ auto_news_intelligence/          # Main pipeline directory
â”‚   â”œâ”€â”€ .env                          # Environment variables (API keys, thresholds)
â”‚   â”œâ”€â”€ requirements.txt              # Python dependencies
â”‚   â”œâ”€â”€ runner.py                     # Main pipeline orchestrator
â”‚   â”œâ”€â”€ download_new_articles.py      # URL to HTML downloader
â”‚   â”œâ”€â”€ dashboard.py                  # Local Streamlit dashboard (deprecated)
â”‚   â”‚
â”‚   â”œâ”€â”€ url_manager.py                # NEW: URL management utilities
â”‚   â”œâ”€â”€ pipeline_runner.py            # NEW: Pipeline runner with live streaming
â”‚   â”œâ”€â”€ app_upload.py                 # NEW: Streamlit admin UI
â”‚   â”œâ”€â”€ verify_installation.py        # NEW: Installation verification script
â”‚   â”‚
â”‚   â”œâ”€â”€ url_batches/                  # URL files directory
â”‚   â”‚   â”œâ”€â”€ all_links.txt             # Master list of article URLs (1204 URLs)
â”‚   â”‚   â”œâ”€â”€ urls.txt                  # Original URL list (518 URLs) - archived
â”‚   â”‚   â”œâ”€â”€ 25_02_links.txt           # Feb 25 URL batch (324 URLs) - archived
â”‚   â”‚   â”œâ”€â”€ 25_02_links_2nd file.txt  # Feb 25 second batch (305 URLs) - archived
â”‚   â”‚   â”œâ”€â”€ 26_02_links.txt           # Feb 26 URL batch (497 URLs) - archived
â”‚   â”‚   â””â”€â”€ combined_urls.txt         # Legacy combined URLs - archived
â”‚   â”‚
â”‚   â”œâ”€â”€ input/
â”‚   â”‚   â””â”€â”€ articles/
â”‚   â”‚       â””â”€â”€ articles/             # Downloaded HTML files (1059 *.html files)
â”‚   â”‚
â”‚   â”œâ”€â”€ output/
â”‚   â”‚   â”œâ”€â”€ .gitkeep                  # Keep directory in git
â”‚   â”‚   â””â”€â”€ results.json              # Final pipeline output (563KB)
â”‚   â”‚
â”‚   â””â”€â”€ pipeline/                     # Core processing modules
â”‚       â”œâ”€â”€ __init__.py               # Package initialization
â”‚       â”œâ”€â”€ html_loader.py            # Extract text from HTML files
â”‚       â”œâ”€â”€ auto_filter.py            # Automobile keyword filtering
â”‚       â”œâ”€â”€ embedder.py               # SBERT embedding generation
â”‚       â”œâ”€â”€ classifier.py             # Category classification
â”‚       â”œâ”€â”€ deduplicator.py           # Duplicate story detection
â”‚       â””â”€â”€ summarizer.py             # GPT-4o-mini summarization (optional)
â”‚
â””â”€â”€ streamlit-app/                    # Deployed dashboard (GitHub repo)
    â”œâ”€â”€ .git/                         # Git repository
    â”œâ”€â”€ .gitignore                    # Git ignore rules
    â”œâ”€â”€ .streamlit/                   # Streamlit cache (auto-generated)
    â”‚
    â”œâ”€â”€ app.py                        # Main Streamlit dashboard application
    â”œâ”€â”€ results.json                  # Pipeline results (copied from auto_news_intelligence/output/)
    â”œâ”€â”€ requirements.txt              # Dashboard dependencies (streamlit, plotly, pandas)
    â”œâ”€â”€ Procfile                      # Heroku deployment config (optional)
    â”œâ”€â”€ README.md                     # Dashboard documentation
    â”‚
    â””â”€â”€ [Deployed at]                 # https://github.com/dark-scientist/auto-news-dashboard
        [Live URL]                    # Streamlit Community Cloud
```

---

## ğŸ”§ Core Files Explained

### **Configuration Files**

#### `.env`
Environment configuration file containing:
- `OPENAI_API_KEY`: OpenAI API key for summarization (optional)
- `INPUT_FOLDER`: Path to HTML articles (default: `input/articles/articles`)
- `SIMILARITY_THRESHOLD`: Deduplication threshold (0.85 = strict, 0.70 = loose)
- `AUTO_FILTER_THRESHOLD`: Automobile filtering threshold (0.40 = strict, 0.25 = loose)
- `MIN_CATEGORY_CONFIDENCE`: Minimum confidence for category assignment (0.20 recommended)

**Example:**
```env
OPENAI_API_KEY=sk-proj-...
INPUT_FOLDER=input/articles/articles
SIMILARITY_THRESHOLD=0.85
AUTO_FILTER_THRESHOLD=0.40
MIN_CATEGORY_CONFIDENCE=0.20
```

#### `requirements.txt`
Python package dependencies:
- `beautifulsoup4` + `lxml`: HTML parsing
- `sentence-transformers`: SBERT embeddings
- `scikit-learn`: Cosine similarity calculations
- `openai`: GPT-4o-mini API (optional)
- `streamlit` + `plotly`: Dashboard visualization
- `python-dotenv`: Environment variable loading

---

### **Pipeline Files**

#### `runner.py` - Main Pipeline Orchestrator
**Purpose**: Coordinates the entire processing pipeline from HTML to JSON output.

**Flow**:
1. Loads environment variables from `.env`
2. Calls `html_loader.py` to extract text from HTML files
3. Calls `auto_filter.py` to filter automobile articles
4. Calls `embedder.py` to generate SBERT embeddings
5. Calls `classifier.py` to categorize articles
6. Applies confidence gate (removes low-confidence articles)
7. Calls `deduplicator.py` to find duplicate stories
8. (Optional) Calls `summarizer.py` to generate summaries
9. Saves final results to `output/results.json`

**Key Functions**:
- `main()`: Orchestrates the entire pipeline
- Logs statistics at each stage (articles processed, filtered, categorized, deduplicated)

**Usage**:
```bash
python runner.py
```

---

#### `download_new_articles.py` - URL Downloader
**Purpose**: Downloads HTML content from URLs and saves to disk.

**How it works**:
1. Reads URLs from `all_links.txt`
2. For each URL:
   - Generates unique filename from URL hash
   - Checks if file already exists (skips if yes)
   - Downloads HTML with proper headers
   - Saves to `input/articles/articles/`
   - Waits 1 second between requests (polite scraping)

**Key Functions**:
- `download_article(url, output_dir)`: Downloads single article
- `main()`: Processes all URLs from file

**Usage**:
```bash
python download_new_articles.py
```

**Output**: HTML files named like `domain_com_hash123.html`

---

### **Pipeline Modules**

#### `pipeline/html_loader.py` - HTML Text Extraction
**Purpose**: Extracts clean article text from HTML files.

**Extraction Strategy** (fallback chain):
1. `<article>` tag
2. `itemprop="articleBody"`
3. Class matching `article-body`, `article-content`
4. Class matching `story-body`, `post-content`
5. `<main>` tag
6. All `<p>` tags with text > 80 chars
7. Nuclear option: all text from page

**Metadata Extraction**:
- **Title**: `og:title` meta tag â†’ `<h1>` â†’ `<title>`
- **URL**: `og:url` meta tag â†’ `<link rel="canonical">`
- **Source**: `og:site_name` meta tag â†’ `<meta name="author">`
- **Date**: `article:published_time` meta tag â†’ `<time datetime>` â†’ current timestamp

**Key Functions**:
- `load_html_articles(folder_path)`: Loads all HTML files from folder
- Returns list of article dictionaries with: `id`, `title`, `content`, `source`, `published_at`, `url`

**Special Handling**:
- Removes `<script>`, `<style>`, `<noscript>`, `<iframe>` tags
- Cleans Times of India language selector boilerplate
- Caps content at 5000 characters
- Skips articles with < 50 characters (likely JS-rendered)

---

#### `pipeline/auto_filter.py` - Automobile Filtering
**Purpose**: Filters articles to keep only automobile-related content.

**Scoring System** (tiered keywords):
- **Tier 1** (0.25 points each): Unambiguous auto signals
  - Stock market names: `ola electric`, `tata motors`, `maruti suzuki`
  - Industry bodies: `siam`, `fada`, `acma`
  - Events: `auto expo`, `bharat mobility`
  - Terms: `automobile`, `automotive industry`, `vehicle recall`

- **Tier 2** (0.15 points each): Strong signals
  - Brands: `tesla`, `toyota`, `bmw`, `mahindra`, `bajaj`
  - Vehicle types: `suv`, `sedan`, `hatchback`, `motorcycle`
  - EV terms: `ev`, `electric vehicle`, `charging station`
  - Tech: `adas`, `autonomous driving`, `connected car`

- **Tier 3** (0.08 points each): Weak signals
  - Generic: `automotive`, `vehicle`, `car`, `fuel`

**Key Functions**:
- `score_article(article)`: Calculates automobile score (0.0 to 1.0)
- `filter_automobile_articles(articles, threshold)`: Filters articles above threshold
- Returns: (automobile_articles, rejected_articles)

**Threshold Guide**:
- `0.25`: Balanced (recommended)
- `0.40`: Strict (fewer false positives)
- `0.15`: Loose (more coverage, more noise)

---

#### `pipeline/embedder.py` - SBERT Embedding Generation
**Purpose**: Generates semantic embeddings for articles using SBERT.

**Model**: `all-MiniLM-L6-v2`
- Dimensions: 384
- Size: ~90MB (downloads on first run)
- Speed: ~1000 articles/second on CPU

**How it works**:
1. Loads pre-trained SBERT model
2. Combines title (2x weight) + content for each article
3. Generates 384-dimensional embedding vector
4. Normalizes vectors for cosine similarity

**Key Functions**:
- `generate_embeddings(articles)`: Generates embeddings for all articles
- Returns: numpy array of shape (n_articles, 384)

**Output**: Dense vector representations enabling semantic similarity comparisons

---

#### `pipeline/classifier.py` - Category Classification
**Purpose**: Classifies articles into 8 fixed categories using zero-LLM approach.

**8 Categories**:
1. **Industry & Market Updates**: Sales, market trends, growth
2. **Regulatory & Policy Updates**: Laws, regulations, compliance
3. **Competitor Activity**: Product launches, pricing, features
4. **Technology & Innovation**: New tech, R&D, innovations
5. **Manufacturing & Operations**: Production, plants, operations
6. **Supply Chain & Logistics**: Supply chain, logistics, distribution
7. **Corporate & Business News**: M&A, partnerships, leadership
8. **External Events**: Events, shows, exhibitions

**How it works** (Zero-LLM):
1. Creates prototype embeddings for each category (from keywords)
2. Calculates cosine similarity between article and all category prototypes
3. Assigns article to category with highest similarity
4. Returns confidence score (0.0 to 1.0)

**Key Functions**:
- `build_category_prototypes()`: Creates category embeddings from keywords
- `classify_articles(articles, embeddings)`: Classifies all articles
- Returns: Articles with `category` and `category_confidence` fields

**Confidence Gate**: Articles with confidence < `MIN_CATEGORY_CONFIDENCE` are removed

---

#### `pipeline/deduplicator.py` - Duplicate Story Detection
**Purpose**: Finds duplicate stories across different sources using clustering.

**Algorithm** (Union-Find + Cosine Similarity):
1. Within each category, compare all article pairs
2. If cosine similarity > `SIMILARITY_THRESHOLD`:
   - Check title entity overlap
   - Check for conflicting brand names
   - If no conflicts, merge into same cluster
3. Use Union-Find data structure for efficient clustering
4. Assign unique `sub_cluster_id` to each story
5. Mark one article as representative (longest content)

**Brand Conflict Detection**:
- Extracts brand names from titles (tesla, toyota, bmw, etc.)
- If two articles mention different brands â†’ NOT duplicates
- If one has brand and other doesn't + similarity < 0.90 â†’ NOT duplicates

**Key Functions**:
- `deduplicate_within_category(articles, embeddings, threshold)`: Clusters articles
- `run_deduplication(articles_by_category, embeddings_by_category)`: Processes all categories
- Returns: Articles with `sub_cluster_id`, `is_representative`, `duplicate_sources`

**Threshold Guide**:
- `0.85`: Strict (fewer false positives, more unique stories)
- `0.75`: Balanced
- `0.65`: Loose (more aggressive clustering)

---

#### `pipeline/summarizer.py` - GPT-4o-mini Summarization
**Purpose**: Generates AI summaries for unique stories (OPTIONAL - currently disabled).

**How it works**:
1. For each unique story (sub_cluster_id):
   - Collects all article titles and content
   - Sends to GPT-4o-mini with prompt
   - Generates 2-3 sentence summary
2. Caches summaries to avoid re-processing

**Key Functions**:
- `summarize_story(articles)`: Generates summary for one story
- `summarize_all_stories(articles_by_category)`: Processes all stories

**Note**: Currently disabled in `runner.py` to save API costs. Enable by uncommenting the summarization section.

---

### **Output Files**

#### `output/results.json` - Final Pipeline Output
**Structure**:
```json
{
  "run_at": "2026-02-26T13:29:37",
  "stats": {
    "total_input": 1013,
    "total_automobile": 436,
    "unique_sources": 27,
    "similarity_threshold": 0.85
  },
  "categories": {
    "Industry & Market Updates": {
      "total_articles": 127,
      "unique_stories": 122,
      "stories": [
        {
          "sub_cluster_id": "sc_000001",
          "story_count": 2,
          "summary": "AI-generated summary...",
          "representative_title": "Article title",
          "sources": ["Source 1", "Source 2"],
          "articles": [
            {
              "id": "art_abc123",
              "title": "Full article title",
              "source": "Source Name",
              "published_at": "2026-02-24T10:30:00",
              "is_representative": true,
              "content_preview": "First 200 chars...",
              "auto_score": 0.95,
              "category_confidence": 0.45,
              "url": "https://..."
            }
          ]
        }
      ]
    }
  }
}
```

**Key Fields**:
- `run_at`: Pipeline execution timestamp
- `stats`: Overall statistics
- `categories`: All 8 categories with their stories
- `sub_cluster_id`: Unique identifier for each story
- `story_count`: Number of sources covering this story
- `is_representative`: Primary article for this story
- `auto_score`: Automobile relevance score (0.0-1.0)
- `category_confidence`: Classification confidence (0.0-1.0)

---

## ğŸ¨ Streamlit Dashboard (`streamlit-app/`)

The `streamlit-app/` directory contains the deployed web dashboard for visualizing pipeline results.

### **Dashboard Files**

#### `app.py` - Main Dashboard Application
**Purpose**: Interactive web dashboard for exploring automobile news intelligence.

**Features**:
1. **Pipeline Metrics Funnel**:
   - Sources â†’ Total Articles â†’ Relevant Articles â†’ Stories â†’ Categories
   - Shows filtering and deduplication statistics
   - Displays duplicate URL count

2. **Latest Headlines**:
   - Live scrolling ticker with recent articles
   - Flashing "LIVE" indicator with timestamp
   - Auto-updates with latest news

3. **Recent News Grid**:
   - 8 articles per page (2 rows Ã— 4 columns)
   - Date range filter (calendar picker)
   - Category filter dropdown
   - Pagination controls
   - Clickable article cards with links

4. **Articles by Source** (Bar Chart):
   - Top 15 sources by article count
   - Colorful horizontal bars
   - Interactive hover tooltips

5. **Articles by Category** (Pie Chart):
   - Donut chart showing category distribution
   - Color-coded by category
   - Percentage labels

6. **Category Breakdown**:
   - List of all categories with article/story counts
   - Color-coded boxes

7. **Story Scatter Plot**:
   - Bubble chart visualization
   - Bubble size = number of sources
   - Color = category
   - Interactive zoom/pan
   - Category filter checkboxes

8. **Detailed Stories by Category**:
   - Expandable story cards
   - AI-generated summaries
   - All articles with preview
   - Source and date information
   - Primary/Duplicate labels
   - Confidence scores

**Key Functions**:
- `load_data()`: Loads results.json (no caching for fresh data)
- `create_scatter_plot(data, selected_categories)`: Generates bubble chart
- `main()`: Orchestrates entire dashboard layout

**Data Flow**:
```
results.json â†’ load_data() â†’ Streamlit components â†’ Interactive UI
```

**Styling**:
- Custom CSS for cards, buttons, and layouts
- Gradient color schemes
- Responsive design
- Hover effects and animations

---

#### `results.json` - Dashboard Data Source
**Purpose**: Copy of pipeline output for dashboard consumption.

**Update Process**:
```bash
# After running pipeline
cp auto_news_intelligence/output/results.json streamlit-app/results.json
git add streamlit-app/results.json
git commit -m "Update dashboard data"
git push origin main
```

**Auto-deployment**: Streamlit Cloud detects changes and redeploys automatically.

---

#### `requirements.txt` - Dashboard Dependencies
**Purpose**: Minimal dependencies for dashboard only.

**Packages**:
```
streamlit==1.31.0      # Dashboard framework
plotly==5.18.0         # Interactive charts
pandas==2.1.4          # Data manipulation
```

**Note**: Much lighter than pipeline requirements (no ML libraries needed).

---

#### `Procfile` - Deployment Configuration
**Purpose**: Heroku deployment configuration (optional).

**Content**:
```
web: streamlit run app.py --server.port=$PORT --server.address=0.0.0.0
```

**Note**: Not required for Streamlit Community Cloud deployment.

---

#### `.gitignore` - Git Ignore Rules
**Purpose**: Excludes cache and temporary files from git.

**Ignored**:
```
.streamlit/          # Streamlit cache
__pycache__/         # Python cache
*.pyc                # Compiled Python
.DS_Store            # macOS files
```

---

#### `README.md` - Dashboard Documentation
**Purpose**: Instructions for deploying and using the dashboard.

**Contents**:
- Deployment instructions for Streamlit Community Cloud
- Local development setup
- Feature overview
- GitHub repository link

---

### **Dashboard Deployment**

#### **Streamlit Community Cloud** (Current)
1. **Repository**: https://github.com/dark-scientist/auto-news-dashboard
2. **Deployment**: Automatic on git push
3. **Hosting**: Free forever (1GB RAM, sleeps after 7 days inactivity)
4. **URL**: Provided by Streamlit Cloud

**Deployment Steps**:
```bash
cd streamlit-app
git add .
git commit -m "Update dashboard"
git push origin main
# Streamlit Cloud auto-deploys in 1-2 minutes
```

**Manual Reboot**: Visit Streamlit Cloud dashboard â†’ Click "Reboot" for instant update

---

#### **Local Development**
```bash
cd streamlit-app
pip install -r requirements.txt
streamlit run app.py
# Opens at http://localhost:8501
```

**Ports**:
- Default: 8501
- Alternative: 8502, 8503 (for testing)

---

### **Dashboard Features Explained**

#### **Dynamic Metrics**
All metrics are calculated from `results.json`:
- **Sources**: Counted from unique article sources (27 sources)
- **Total Articles**: From stats.total_input (1013 articles)
- **Relevant Articles**: From stats.total_automobile (436 articles)
- **Stories**: Sum of unique_stories across categories (413 stories)
- **Duplicates**: Calculated as relevant_articles - stories (23 duplicates)
- **Irrelevant**: Calculated as total_articles - relevant_articles (577 filtered)

#### **Date Handling**
- Articles dated Feb 24-26, 2026 (145, 145, 146 distribution)
- Date filter allows custom range selection
- Default range: 2018-01-01 to 2026-02-26
- Dates extracted from HTML metadata or set to processing date

#### **Category Colors**
```python
CATEGORY_COLORS = {
    "Industry & Market Updates": "#3B82F6",      # Blue
    "Regulatory & Policy Updates": "#8B5CF6",    # Purple
    "Competitor Activity": "#F59E0B",            # Orange
    "Technology & Innovation": "#10B981",        # Green
    "Manufacturing & Operations": "#EF4444",     # Red
    "Supply Chain & Logistics": "#F97316",       # Dark Orange
    "Corporate & Business News": "#06B6D4",      # Cyan
    "External Events": "#6B7280"                 # Gray (removed in current version)
}
```

#### **Scatter Plot Algorithm**
1. Groups stories by category
2. Assigns random X/Y positions with category clustering
3. Bubble size = 15 + (story_count Ã— 8), capped at 60
4. Hover shows: title, category, sources, summary
5. Interactive: pan, zoom, filter by category

---

### **Dashboard vs Pipeline**

| Feature | Pipeline | Dashboard |
|---------|----------|-----------|
| **Purpose** | Process articles | Visualize results |
| **Input** | HTML files | results.json |
| **Output** | results.json | Interactive UI |
| **Dependencies** | ML libraries | Streamlit, Plotly |
| **Runtime** | ~20 seconds | Instant |
| **Deployment** | Local only | Cloud-hosted |
| **Updates** | Manual run | Auto-deploy on git push |

---

### **Updating Dashboard Data**

**Workflow**:
```bash
# 1. Run pipeline
cd auto_news_intelligence
python runner.py

# 2. Copy results
cp output/results.json ../streamlit-app/results.json

# 3. Deploy
cd ../streamlit-app
git add results.json
git commit -m "Update with latest articles"
git push origin main

# 4. Wait 1-2 minutes for auto-deployment
```

**Verification**:
- Check Streamlit Cloud dashboard for deployment status
- Refresh browser to see updated data
- Verify article counts and dates

---

## ğŸš€ Usage

### Option A: New Streamlit Admin UI (Recommended)
```bash
# Launch the admin interface
cd auto_news_intelligence
streamlit run app_upload.py
```

The admin UI provides:
- **URL Upload**: Upload .txt files with URLs, preview new vs existing, add to url_batches/all_links.txt
- **Pipeline Execution**: Run full pipeline with live log streaming and progress tracking
- **Results Export**: View metrics, download results.json, copy to streamlit-app/

### Option B: Manual Command Line
```bash
# 1. Add URLs to url_batches/all_links.txt (one per line)
python download_new_articles.py

# 2. Process all HTML files
python runner.py

# 3. View results
streamlit run dashboard.py
```

Or check `output/results.json` directly.

---

## âš™ï¸ Configuration Guide

### Threshold Tuning

**SIMILARITY_THRESHOLD** (Deduplication):
- `0.90`: Very strict - only exact duplicates
- `0.85`: Strict - recommended for clean clustering
- `0.75`: Balanced - some false positives
- `0.65`: Loose - aggressive clustering

**AUTO_FILTER_THRESHOLD** (Automobile Filtering):
- `0.40`: Strict - only clear automobile content
- `0.25`: Balanced - recommended
- `0.15`: Loose - includes tangential content

**MIN_CATEGORY_CONFIDENCE** (Classification):
- `0.25`: Strict - only confident classifications
- `0.20`: Balanced - recommended
- `0.14`: Loose - includes uncertain articles

---

## ğŸ“Š Pipeline Statistics

**Example Run** (1204 URLs):
```
INPUT:       1013 total articles (191 duplicate URLs removed)
AUTO FILTER: 436 automobile articles (577 irrelevant filtered)
CATEGORIES:  7 categories (External Events removed)
STORIES:     413 unique stories (23 duplicates found)
SOURCES:     27 unique news sources
```

**Processing Time**:
- HTML Loading: ~5 seconds
- Auto Filtering: ~2 seconds
- Embedding: ~10 seconds (first run: +30s for model download)
- Classification: ~1 second
- Deduplication: ~3 seconds
- **Total: ~20 seconds** (for 1000 articles)

---

## ğŸ¯ Key Features

### Zero-LLM Approach
- **Classification**: Pure cosine similarity (no LLM calls)
- **Deduplication**: Cosine similarity + Union-Find (no LLM calls)
- **Summarization**: Optional GPT-4o-mini (only for final summaries)

### Scalability
- Handles 10 to 10,000+ articles
- Linear time complexity O(n)
- Deduplication is O(nÂ²) per category but parallelizable

### Accuracy
- Auto-filtering: ~95% precision with threshold 0.40
- Classification: ~85% accuracy (zero-LLM)
- Deduplication: ~90% precision with threshold 0.85

---

## ğŸ› Troubleshooting

**Issue**: "No articles loaded"
- Check HTML files exist in `input/articles/articles/`
- Verify HTML files have content (not empty)

**Issue**: "All articles filtered out"
- Lower `AUTO_FILTER_THRESHOLD` in `.env`
- Check if articles are actually automobile-related

**Issue**: "Too many duplicate clusters"
- Increase `SIMILARITY_THRESHOLD` in `.env`
- Check brand conflict detection is working

**Issue**: "SBERT model download fails"
- Check internet connection
- Manually download model: `sentence-transformers/all-MiniLM-L6-v2`

---

## ğŸ“ Notes

- First run downloads SBERT model (~90MB)
- Summarization is optional (disabled by default to save costs)
- All duplicate articles are preserved (not deleted)
- Works offline after model download (except summarization)
- HTML files are never modified or deleted

---

## ğŸ”— Related Files

- **Streamlit Dashboard**: See `../streamlit-app/` for deployed dashboard
- **GitHub Repo**: https://github.com/dark-scientist/auto-news-dashboard
- **Live Dashboard**: Deployed on Streamlit Community Cloud

---

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA FLOW DIAGRAM                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. URL Collection
   â”œâ”€â”€ Historical batches (in url_batches/)
   â”‚   â”œâ”€â”€ urls.txt (518)
   â”‚   â”œâ”€â”€ 25_02_links.txt (324)
   â”‚   â”œâ”€â”€ 25_02_links_2nd file.txt (305)
   â”‚   â””â”€â”€ 26_02_links.txt (497)
   â†“
   url_batches/all_links.txt (1204 unique URLs)

2. HTML Download
   download_new_articles.py
        â†“
   input/articles/articles/*.html (1059 files)

3. Pipeline Processing
   runner.py orchestrates:
        â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ html_loader.py                      â”‚ â†’ 1013 articles
   â”‚ auto_filter.py                      â”‚ â†’ 436 automobile
   â”‚ embedder.py                         â”‚ â†’ 384-dim vectors
   â”‚ classifier.py                       â”‚ â†’ 7 categories
   â”‚ deduplicator.py                     â”‚ â†’ 413 unique stories
   â”‚ summarizer.py (optional)            â”‚ â†’ AI summaries
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
   output/results.json (563KB)

4. Dashboard Deployment
   cp output/results.json â†’ streamlit-app/results.json
        â†“
   git push â†’ GitHub
        â†“
   Streamlit Cloud auto-deploy
        â†“
   Live Dashboard (Web UI)
```

---

## ğŸ“‚ Directory Separation

### **Why Two Directories?**

**`auto_news_intelligence/`** (Pipeline):
- Heavy ML dependencies (sentence-transformers, scikit-learn)
- Runs locally on your machine
- Processes raw HTML files
- Generates results.json
- Not deployed to cloud

**`streamlit-app/`** (Dashboard):
- Lightweight dependencies (streamlit, plotly, pandas)
- Deployed to Streamlit Community Cloud
- Reads results.json only
- No processing, just visualization
- Separate git repository for clean deployment

### **Benefits**:
1. **Faster Deployment**: Dashboard deploys in seconds (no ML libraries)
2. **Cost Efficiency**: No need to run pipeline in cloud
3. **Flexibility**: Update pipeline without redeploying dashboard
4. **Security**: API keys stay local, not in cloud
5. **Scalability**: Pipeline can run on powerful local machine

---

## ğŸ”„ Complete Workflow

### **Initial Setup**
```bash
# 1. Clone/setup pipeline
cd auto_news_intelligence
pip install -r requirements.txt
cp .env.example .env  # Add your OpenAI API key

# 2. Clone/setup dashboard
cd ../streamlit-app
pip install -r requirements.txt
git remote add origin https://github.com/dark-scientist/auto-news-dashboard.git
```

### **Regular Updates**
```bash
# 1. Add new URLs
echo "https://example.com/article" >> auto_news_intelligence/url_batches/all_links.txt

# 2. Download articles
cd auto_news_intelligence
python download_new_articles.py

# 3. Run pipeline
python runner.py

# 4. Update dashboard
cp output/results.json ../streamlit-app/results.json
cd ../streamlit-app
git add results.json
git commit -m "Update: $(date +%Y-%m-%d)"
git push origin main

# 5. Verify deployment (wait 1-2 minutes)
# Visit Streamlit Cloud URL
```

### **Troubleshooting Workflow**
```bash
# Pipeline issues
cd auto_news_intelligence
python runner.py  # Check logs

# Dashboard issues
cd streamlit-app
streamlit run app.py  # Test locally first
git push origin main  # Deploy if local works
```

---

## ğŸ”— Related Files

- **Streamlit Dashboard**: See `../streamlit-app/` for deployed dashboard
- **GitHub Repo**: https://github.com/dark-scientist/auto-news-dashboard
- **Live Dashboard**: Deployed on Streamlit Community Cloud

---

## ğŸ“¦ Requirements

- Python 3.8+
- 500MB disk space (SBERT model)
- OpenAI API key (optional, for summarization)
- Internet connection (first run only)

---

## ğŸ“ Technical Details

**Embedding Model**: `all-MiniLM-L6-v2`
- Architecture: Sentence-BERT
- Dimensions: 384
- Training: Trained on 1B+ sentence pairs
- Performance: 0.68 Spearman correlation on STS benchmark

**Classification Method**: Zero-shot cosine similarity
- No training required
- Category prototypes from keyword embeddings
- Fast and interpretable

**Deduplication Algorithm**: Union-Find clustering
- Time: O(nÂ²) comparisons per category
- Space: O(n) for Union-Find structure
- Optimizations: Early stopping, brand conflict detection

---

## ğŸ“„ License

MIT License - See LICENSE file for details

---

## ğŸ†• New Features (v2.0)

### URL Management System
**File**: `url_manager.py`

Consolidates scattered URL files into a single master list with intelligent deduplication:

**Functions**:
- `load_master_urls()`: Loads existing URLs from all_links.txt
- `parse_urls_from_text(text)`: Extracts valid HTTP/HTTPS URLs from raw text
- `append_new_urls(new_urls)`: Deduplicates and appends only new URLs

**Benefits**:
- Single source of truth (all_links.txt)
- Automatic deduplication
- Append-only (never overwrites)
- Timestamped logging

### Pipeline Runner with Live Streaming
**File**: `pipeline_runner.py`

Executes the full pipeline with real-time log streaming for UI integration:

**Function**:
- `stream_pipeline()`: Generator that yields log lines and returns final stats

**Features**:
- Runs download_new_articles.py then runner.py sequentially
- Captures stdout/stderr line by line
- Stage-labeled output ([DOWNLOAD], [LOAD], [FILTER], etc.)
- Parses final statistics from logs and results.json
- Graceful error handling with detailed messages

### Streamlit Admin UI
**File**: `app_upload.py`

New operator/admin interface for managing the pipeline:

**Section 1: URL Upload**
- File uploader for .txt files
- Preview table showing New vs Already exists status
- Counts: X new URLs, Y already in master
- Confirm & Add button with success toast

**Section 2: Run Pipeline**
- Current stats: total URLs, HTML files, last run timestamp
- Run Full Pipeline button
- Live log stream with stage labels
- Progress bar advancing through known stages
- Real-time updates

**Section 3: Output & Export**
- Summary metrics in cards: Total Articles, Auto-relevant, Categories, Unique Stories, Sources
- Category breakdown table
- Preview results.json (first 50 lines)
- Download results.json button
- Copy to streamlit-app/ button
- Git instructions for deployment

**Sidebar**:
- View current .env configuration
- Threshold values display

### Enhanced Deduplication
**File**: `pipeline/deduplicator.py`

Improved clustering with coherence checking and stray detection:

**New Features**:
1. **Cluster Coherence Scoring**
   - Computes average pairwise similarity within each cluster
   - Warns if coherence < 0.70 (may be mis-grouped)
   - Adds `cluster_coherence_score` field to every article

2. **Stray Article Detection**
   - For each non-representative article, checks similarity to representative
   - If similarity < 0.78 AND zero shared entities â†’ removed from cluster
   - Creates singleton cluster for strays
   - Entity detection: brands, capitalized words 5+ chars, 4-digit numbers

3. **Duplicate Reason Field**
   - Adds `duplicate_reason` to non-representative articles
   - Format: "Similar to: '[rep title, max 45 chars]...' (sim={score:.2f})"

**Functions**:
- `extract_named_entities(text)`: Extracts brands, proper nouns, numbers
- `compute_cluster_coherence(indices, sim_matrix)`: Calculates cluster quality
- Enhanced `deduplicate_within_category()`: Implements all improvements

### Classification Explanations
**File**: `pipeline/classifier.py`

Adds human-readable explanations for category assignments:

**New Feature**:
- `cluster_reason` field added to every article
- Explains why article was placed in its category

**Algorithm**:
1. Finds which keywords from category prototype appear in article
2. Takes top 3 matching keywords
3. Format: "Matched on: [kw1], [kw2], [kw3]"
4. Fallback: "Best semantic match to [category] (score={confidence:.2f})"

**Function**:
- `_generate_cluster_reason(article, category, confidence)`: Generates explanation

### Enhanced Output Schema
**File**: `runner.py` - `build_output()`

Updated article objects in results.json now include:

```json
{
  "id": "art_abc123",
  "title": "Article title",
  "source": "Source Name",
  "published_at": "2026-02-24T10:30:00",
  "is_representative": true,
  "content_preview": "First 200 chars...",
  "auto_score": 0.95,
  "category_confidence": 0.45,
  "url": "https://...",
  "cluster_reason": "Matched on: sales, market, growth",
  "duplicate_reason": "Similar to: 'Tesla Q4 earnings...' (sim=0.89)",
  "cluster_coherence_score": 0.87
}
```

Story objects also include:
```json
{
  "sub_cluster_id": "sc_000001",
  "story_count": 3,
  "cluster_coherence_score": 0.87,
  "articles": [...]
}
```

**Backward Compatibility**:
- All new fields use `.get()` with safe defaults
- Old results.json files won't break existing dashboard

### Workflow Improvements

**Old Workflow**:
```bash
# Manual, scattered
1. Collect URLs â†’ append to various files
2. python download_new_articles.py
3. python runner.py
4. cp output/results.json ../streamlit-app/results.json
5. cd ../streamlit-app && git push
```

**New Workflow**:
```bash
# Streamlined via UI
1. streamlit run app_upload.py
2. Upload URL file â†’ preview â†’ confirm
3. Click "Run Full Pipeline" â†’ watch live logs
4. Click "Copy to streamlit-app/" â†’ follow git instructions
```

**Benefits**:
- Single interface for all operations
- Visual feedback at every step
- No command-line knowledge required
- Live progress tracking
- Error handling with clear messages

---

## ğŸ”§ Technical Improvements

### Deduplication Quality
- **Before**: Basic cosine similarity clustering
- **After**: Coherence-aware clustering with stray detection
- **Impact**: Fewer mis-grouped articles, cleaner clusters

### Explainability
- **Before**: Black-box classification and clustering
- **After**: Every decision explained with `cluster_reason` and `duplicate_reason`
- **Impact**: Easier debugging, better trust in results

### Operator Experience
- **Before**: Command-line only, manual file management
- **After**: GUI with live feedback, automated workflows
- **Impact**: Faster iterations, fewer errors

### Data Quality
- **Before**: Scattered URL files, manual deduplication
- **After**: Single master list, automatic deduplication
- **Impact**: No duplicate downloads, cleaner data lineage

---

## ğŸ“Š New Metrics Available

### Cluster Coherence Score
- Range: 0.0 to 1.0
- Interpretation:
  - > 0.85: Very tight cluster (high confidence)
  - 0.70-0.85: Good cluster
  - < 0.70: Loose cluster (warning logged)

### Duplicate Reason
- Shows which article this is similar to
- Includes similarity score
- Helps understand clustering decisions

### Cluster Reason
- Shows why article was classified into category
- Lists matching keywords or semantic score
- Aids in debugging misclassifications

---

## ğŸ¯ Use Cases for New Features

### URL Management
- **Scenario**: Receive daily URL batches from multiple sources
- **Solution**: Upload each batch via UI, automatic deduplication
- **Benefit**: No manual merging, no duplicate downloads

### Pipeline Monitoring
- **Scenario**: Need to track pipeline progress and catch errors
- **Solution**: Live log streaming with stage labels
- **Benefit**: Immediate feedback, faster debugging

### Quality Assurance
- **Scenario**: Need to verify clustering and classification quality
- **Solution**: Check cluster_coherence_score and cluster_reason fields
- **Benefit**: Data-driven quality metrics, explainable decisions

### Deployment Automation
- **Scenario**: Update dashboard after pipeline runs
- **Solution**: One-click copy to streamlit-app/ with git instructions
- **Benefit**: Faster deployments, fewer manual steps

---

## ğŸ”„ Migration Guide

### From v1.0 to v2.0

**No breaking changes** - v2.0 is fully backward compatible.

**Optional Upgrades**:
1. Start using `app_upload.py` instead of command-line
2. Check new fields in results.json for insights
3. Use `url_manager.py` for programmatic URL management

**Existing Workflows**:
- All v1.0 scripts still work unchanged
- Old results.json files work with new code
- No re-processing required

**New Capabilities**:
- Run `streamlit run app_upload.py` to try new UI
- Inspect `cluster_coherence_score` in results.json
- Read `cluster_reason` and `duplicate_reason` for explanations

---

## ğŸ“„ License

MIT License - See LICENSE file for details
